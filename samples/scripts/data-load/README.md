# Auto Generate Azure Synapse Analytics SQL Pool (formerly SQL Data Warehouse)Load – TSQL Scripts

These scripts were developed to help with a large Azure Synapse Analytics SQL Pool (formerly SQL Data Warehouse) POC with one of the DMJ customers. Some of the assumptions made in the scripts are very specific to the scenario encountered at the customer. Feel free to adapt these scripts to your data loading scenario.

Each of the scripts creates the appropriate objects to allow an optimal performance DW load from text files in blob storage into Azure SQL DW using Polybase. You can easily change the external data source to use Azure Data Lake as a source as well. The optimal nature of the load will depend on the number of files provided. Our customer had many gzip’d files that distributed well among several DW nodes (few larger gzip’d files would not have done as well).

From a workflow perspective, we used SSMA for Oracle to generate the DW schema and then manually tweaked the data types in the script (the customer used the “number” data type without scale or precision frequently). The SSMA default conversion of number to float(53) needed to be adjusted (given you are converting from precise to approximate). Part of the manual schema editing process was also deciding on distribution hash keys, which was an initial guess, since we didn’t have a list of typical queries. If we had no idea what to hash on, we left the tables round robin. For dimension tables, this would also be an appropriate time to make them replicated tables. We did not have row counts or sizes at that point, so we couldn’t make that determination (the guidance is a maximum size of 2 GB). 

Before running the schema script on the DW, we put all the tables into “source” schemas (SRC_XXX). The “source” schema is the template that the script uses to create the final tables in the target schema (including column types and distribution – note that no other indexes are created, beyond the default CCI index). Once a table is processed, just remove it from the source schema. We did not add this step to the script just in case there were errors that didn’t stop the script, but the drop could be easily added. 

In the POC, we had 72 tables split across 4 schemas. We could load each schema separately with these scripts (note; to load multiple schemas in parallel with a script would require you to give the AzureStorage object a unique name for each copy of the script).

During the POC, we had two locations where the customer put the text files to be loaded in Azure Blob Storage. Initially, they put the files in their own container under the same storage account. Many of the tables in their schema had underscore characters in the name, but the container names in blob storage had the table names with dashes instead of underscores (so the script has a replace to change this automatically). Later during the POC, the customer started moving newly extracted tables to a single container with a subfolder naming scheme (note; in blob store there aren’t actually subfolders, just a naming trick). There are therefore two different script versions, one for each case. In the first case we need to create a separate external data source for each container (we couldn’t get the root container syntax to work), while in the second case we can use the same external data source for every table and specify the subfolder in the external table creation command.

The V1 script generates a set of “classic” load statements, where the external table definition has the appropriate types expected in each column. The CTAS statement then is just a simple “create table … select *…”.  Also included is a single table CTAS load script that documents every step required to load one or more text files into a table in Azure SQL DW.

During the POC, we started with the V1 script, but quickly determined that the script failed to load many of the customer tables for a variety of reasons, including; every column even numeric ones were quoted (which Polybase might have handled, except null values were specified as “null”), timestamps had too much precision for Polybase, embedded escaped quotes (\”) were not handled by Polybase, embedded new line characters caused Polybase to error etc.

A strategy was developed to address some of the loading issues, which in some cases required the files to be re-extracted from the source system (hence the change during the POC of where files were stored). The customer originally had selected an ASCII 01 (SOH) character as the record separator. This seemed like a good choice, since, being an infrequently used control character, it should not appear in any of the text fields. The hope was that Polybase would not have an issue with it, and there wasn’t once we specified the value in the script in hex. 

Given the relatively exotic column separator, there really wasn’t a need for quotes around strings, but they were already in the generated files. We ended up having to remove the quote specification from the external file format definition, because the embedded quotes in columns were causing Polybase errors. In the process we switched to reading everything from the text files as character columns (or Unicode, if required) to reduce the number of Polybase issues we were seeing. Note that since we are reading everything into character columns and there may be embedded quotes in the strings, the character column lengths used are arbitrarily longer than the actual source columns length (we settled on adding 20 characters to character columns’ length).

Instead of staging the raw file in a temp table, we came up with a strategy to cast the columns in the CTAS statement, to the appropriate data types (later we discovered the trick to add nullability). The complexity of the CTAS statement grew over time to handle many issues that we ran into. For some of the tables, the statements were longer than a print command could handle (so it is printed in multiple lines, with no concept of an appropriate line break location). 

One issue that was very problematic was newlines embedded in column text. We had the customer regenerate the files using an ascii 31 (RS) for an ascii 10 (LF) and an ascii 30 (US) for an ascii 13 (CR) and we added code to handle the replacement. They initially missed some columns with both CR and LF as newlines, but eventually they got them all. One table had a SOH character that showed up in one of the columns, which was very problematic to hunt down – the error was a string truncation but in column 85, but the problem column was actually 32. We had the customer regenerate the files for this table with a different record separator of an ascii 28 (FS) – this was a temporary script change that was not shared in this package but would be easy to replicate.

If the script fails, you will need to manually remove the external table definition created (drop external table…). We could remove these tables automatically in the script but have no idea if there are multiple scripts running in parallel.


Any comments and questions can be directed to the script author: Mitch van Huuksloot, Solution Architect, Data Migration Jumpstart Engineering Team (email is name with .'s @ microsoft.com)
